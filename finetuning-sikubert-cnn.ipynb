{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec8812a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:33:45.296885Z",
     "iopub.status.busy": "2026-02-01T12:33:45.296344Z",
     "iopub.status.idle": "2026-02-01T12:33:50.258432Z",
     "shell.execute_reply": "2026-02-01T12:33:50.257569Z"
    },
    "papermill": {
     "duration": 4.972748,
     "end_time": "2026-02-01T12:33:50.260478",
     "exception": false,
     "start_time": "2026-02-01T12:33:45.287730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers torch datasets scikit-learn tqdm accelerate optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672dbc59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:33:50.273866Z",
     "iopub.status.busy": "2026-02-01T12:33:50.273617Z",
     "iopub.status.idle": "2026-02-01T12:33:50.279071Z",
     "shell.execute_reply": "2026-02-01T12:33:50.278500Z"
    },
    "papermill": {
     "duration": 0.013717,
     "end_time": "2026-02-01T12:33:50.280392",
     "exception": false,
     "start_time": "2026-02-01T12:33:50.266675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "import os\n",
    "\n",
    "LOG_DIR = \"/kaggle/working\"\n",
    "LOG_FILE = os.path.join(LOG_DIR, \"train.log\")\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, mode=\"w\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler(sys.stdout),\n",
    "    ],\n",
    "    force=True,   # <<< QUAN TRỌNG\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e58713",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:33:50.292886Z",
     "iopub.status.busy": "2026-02-01T12:33:50.292466Z",
     "iopub.status.idle": "2026-02-01T12:33:54.587125Z",
     "shell.execute_reply": "2026-02-01T12:33:54.586390Z"
    },
    "papermill": {
     "duration": 4.302639,
     "end_time": "2026-02-01T12:33:54.588635",
     "exception": false,
     "start_time": "2026-02-01T12:33:50.285996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Setup\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "\n",
    "def set_seed(seed=42): # Cố định random seed để kết quả reproducible\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "logger.info(f\"✓ Setup complete. Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"  GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab51635",
   "metadata": {
    "papermill": {
     "duration": 0.005657,
     "end_time": "2026-02-01T12:33:54.600174",
     "exception": false,
     "start_time": "2026-02-01T12:33:54.594517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Thiết kế chính\n",
    "\n",
    "- TaskConfig: Encapsulate toàn bộ thông tin về label schema\n",
    "- Dễ dàng thêm tác vụ mới bằng cách tạo config mới\n",
    "- ignore_labels: Linh hoạt định nghĩa labels không tính trong evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc428ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:33:54.612744Z",
     "iopub.status.busy": "2026-02-01T12:33:54.612376Z",
     "iopub.status.idle": "2026-02-01T12:33:54.623171Z",
     "shell.execute_reply": "2026-02-01T12:33:54.622461Z"
    },
    "papermill": {
     "duration": 0.018889,
     "end_time": "2026-02-01T12:33:54.624595",
     "exception": false,
     "start_time": "2026-02-01T12:33:54.605706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class TaskConfig:\n",
    "    \"\"\"Configuration cho mỗi tác vụ\"\"\"\n",
    "    task_name: str\n",
    "    labels: List[str]\n",
    "    label2id: Dict[str, int]\n",
    "    id2label: Dict[int, str]\n",
    "    num_labels: int\n",
    "    ignore_labels: List[str] = None  # Labels bỏ qua khi eval\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, task_name: str, labels: List[str], ignore_labels: List[str] = None):\n",
    "        \"\"\"Factory method tạo config\"\"\"\n",
    "        label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "        id2label = {idx: label for label, idx in label2id.items()}\n",
    "        return cls(\n",
    "            task_name=task_name,\n",
    "            labels=labels,\n",
    "            label2id=label2id,\n",
    "            id2label=id2label,\n",
    "            num_labels=len(labels),\n",
    "            ignore_labels=ignore_labels or []\n",
    "        )\n",
    "\n",
    "# Config cho Sentence Punctuation\n",
    "PUNCTUATION_CONFIG = TaskConfig.create(\n",
    "    task_name=\"punctuation\",\n",
    "    labels=['O', '，', '。', '：', '、', '；', '？', '！'],\n",
    "    ignore_labels=['O']  # Bỏ qua token không có dấu khi eval\n",
    ")\n",
    "\n",
    "# Config cho Sentence Segmentation\n",
    "SEGMENTATION_CONFIG = TaskConfig.create(\n",
    "    task_name=\"segmentation\",\n",
    "    labels=['B', 'M', 'E', 'S'],\n",
    "    ignore_labels=[]  # Đánh giá tất cả các nhãn\n",
    ")\n",
    "\n",
    "# Training hyperparameters\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    \"\"\"Hyperparameters chung\"\"\"\n",
    "    model_name: str = \"SIKU-BERT/sikubert\"\n",
    "    max_length: int = 256\n",
    "\n",
    "    # Hyperparameters to be tuned\n",
    "    batch_size: int = 64\n",
    "    learning_rate: float = 2e-5\n",
    "    num_epochs: int = 2\n",
    "    warmup_ratio: float = 0.1\n",
    "    weight_decay: float = 0.01\n",
    "    dropout: float = 0.1\n",
    "    max_grad_norm: float = 1.0\n",
    "\n",
    "    # Early stopping\n",
    "    early_stopping_patience: int = 2 # Kết quả val ko tăng 3 lần liên tiếp\n",
    "\n",
    "    # Fixed\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    seed: int = 42\n",
    "    gradient_accumulation_steps: int = 32  # Tăng nếu GPU memory không đủ\n",
    "    fp16=True\n",
    "\n",
    "logger.info(\"✅ Configurations defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44922ebd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:33:54.637958Z",
     "iopub.status.busy": "2026-02-01T12:33:54.637725Z",
     "iopub.status.idle": "2026-02-01T12:33:54.641450Z",
     "shell.execute_reply": "2026-02-01T12:33:54.640747Z"
    },
    "papermill": {
     "duration": 0.011775,
     "end_time": "2026-02-01T12:33:54.642895",
     "exception": false,
     "start_time": "2026-02-01T12:33:54.631120",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 1. Select task\n",
    "TASK = \"segmentation\"  # or \"punctuation\"\n",
    "task_config = PUNCTUATION_CONFIG if TASK == \"punctuation\" else SEGMENTATION_CONFIG\n",
    "train_path='/kaggle/input/createsegdataset256-sliding/segmentation_sliding_part_0.jsonl'\n",
    "val_path='/kaggle/input/createsegdataset256-sliding/segmentation_sliding_part_8.jsonl'\n",
    "test_path='/kaggle/input/createsegdataset256-sliding/segmentation_sliding_part_9.jsonl'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a29063",
   "metadata": {
    "papermill": {
     "duration": 0.005692,
     "end_time": "2026-02-01T12:33:54.654536",
     "exception": false,
     "start_time": "2026-02-01T12:33:54.648844",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dataset & Preprocessing\n",
    "- Hỗ trợ character-level alignment (quan trọng cho Classical Chinese)\n",
    "- Xử lý special tokens ([CLS], [SEP], [PAD]) bằng label -100\n",
    "- Validate input để phát hiện lỗi sớm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6530b36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:33:54.667216Z",
     "iopub.status.busy": "2026-02-01T12:33:54.666960Z",
     "iopub.status.idle": "2026-02-01T12:34:06.339571Z",
     "shell.execute_reply": "2026-02-01T12:34:06.338744Z"
    },
    "papermill": {
     "duration": 11.68099,
     "end_time": "2026-02-01T12:34:06.341197",
     "exception": false,
     "start_time": "2026-02-01T12:33:54.660207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "\n",
    "class ClassicalChineseDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset cho token classification tasks.\n",
    "    \n",
    "    Input format:\n",
    "        texts: List[str] - danh sách văn bản (mỗi văn bản là chuỗi ký tự)\n",
    "        labels: List[List[str]] - nhãn tương ứng cho mỗi ký tự\n",
    "    \n",
    "    Example:\n",
    "        texts = [\"天地玄黃\", \"宇宙洪荒\"]\n",
    "        labels = [['B', 'M', 'M', 'E'], ['B', 'M', 'M', 'E']]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        texts: List[str],\n",
    "        labels: List[List[str]],\n",
    "        tokenizer,\n",
    "        config: TaskConfig,\n",
    "        max_length: int = 256\n",
    "    ):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.config = config\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Validate data\n",
    "        assert len(texts) == len(labels), \"texts và labels phải cùng độ dài\"\n",
    "        for text, label_seq in zip(texts, labels):\n",
    "            assert len(text) == len(label_seq), \\\n",
    "                f\"Text và labels không khớp: {len(text)} vs {len(label_seq)}\"\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label_seq = self.labels[idx]\n",
    "        \n",
    "        # Tokenize với is_split_into_words=True để track alignment\n",
    "        # SikuBERT thường tokenize từng ký tự -> 1:1 mapping\n",
    "        tokenized = self.tokenizer(\n",
    "            list(text),  # Convert sang list ký tự\n",
    "            is_split_into_words=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Align labels với subword tokens\n",
    "        # SikuBERT: thường 1 char = 1 token, nhưng vẫn cần xử lý edge cases\n",
    "        word_ids = tokenized.word_ids(batch_index=0)\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                # [CLS], [SEP], [PAD] -> assign -100 (ignored by CrossEntropyLoss)\n",
    "                label_ids.append(-100)\n",
    "            else:\n",
    "                # Map label sang ID\n",
    "                label = label_seq[word_id]\n",
    "                label_ids.append(self.config.label2id[label])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': tokenized['input_ids'].squeeze(0),\n",
    "            'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_texts: List[str],\n",
    "    train_labels: List[List[str]],\n",
    "    val_texts: List[str],\n",
    "    val_labels: List[List[str]],\n",
    "    tokenizer,\n",
    "    config: TaskConfig,\n",
    "    training_config: TrainingConfig\n",
    ") -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"Factory function tạo train & val dataloaders\"\"\"\n",
    "    \n",
    "    train_dataset = ClassicalChineseDataset(\n",
    "        train_texts, train_labels, tokenizer, config, training_config.max_length\n",
    "    )\n",
    "    val_dataset = ClassicalChineseDataset(\n",
    "        val_texts, val_labels, tokenizer, config, training_config.max_length\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=training_config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=2,\n",
    "        pin_memory=True,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "logger.info(\"✅ Dataset class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c5ca6",
   "metadata": {
    "papermill": {
     "duration": 0.006169,
     "end_time": "2026-02-01T12:34:06.353849",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.347680",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Definition\n",
    "\n",
    "- Module hóa: Dễ dàng thêm BiLSTM/CNN qua parameter extra_layer_type\n",
    "- Extensible: Placeholder cho CRF (sẽ return logits, CRF xử lý bên ngoài)\n",
    "- Linear head đơn giản nhưng hiệu quả cho baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b764e702",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:06.367843Z",
     "iopub.status.busy": "2026-02-01T12:34:06.367426Z",
     "iopub.status.idle": "2026-02-01T12:34:06.403022Z",
     "shell.execute_reply": "2026-02-01T12:34:06.402369Z"
    },
    "papermill": {
     "duration": 0.04443,
     "end_time": "2026-02-01T12:34:06.404390",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.359960",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SikuBERTForTokenClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    SikuBERT với classification head có thể mở rộng.\n",
    "    \n",
    "    Architecture:\n",
    "        BERT Encoder -> [Optional: Extra Layers] -> Classification Head\n",
    "    \n",
    "    Thiết kế module hóa cho phép:\n",
    "        - Thay Linear head bằng CRF\n",
    "        - Thêm BiLSTM/CNN layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str,\n",
    "        num_labels: int,\n",
    "        dropout: float = 0.1,\n",
    "        use_extra_layer: bool = False,\n",
    "        extra_layer_type: str = None,  # 'lstm', 'cnn', None\n",
    "        cnn_kernel_sizes: list = None,  # Mới: kernel sizes cho CNN\n",
    "        cnn_num_filters: int = 128      # Mới: số filters cho CNN\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Backbone: SikuBERT\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Extra layers (placeholder cho future extensions)\n",
    "        self.extra_layer = None\n",
    "        self.extra_layer_type = extra_layer_type\n",
    "        \n",
    "        if use_extra_layer:\n",
    "            if extra_layer_type == 'lstm':\n",
    "                # BiLSTM layer\n",
    "                self.extra_layer = nn.LSTM(\n",
    "                    self.hidden_size,\n",
    "                    self.hidden_size // 2,\n",
    "                    batch_first=True,\n",
    "                    bidirectional=True\n",
    "                )\n",
    "                classifier_input_size = self.hidden_size\n",
    "                \n",
    "            elif extra_layer_type == 'cnn':\n",
    "                # CNN layer với multiple kernel sizes\n",
    "                if cnn_kernel_sizes is None:\n",
    "                    cnn_kernel_sizes = [3, 5, 7]  # default\n",
    "                \n",
    "                self.cnn_kernel_sizes = cnn_kernel_sizes\n",
    "                self.cnn_num_filters = cnn_num_filters\n",
    "                \n",
    "                # Tạo multiple Conv1d layers với kernel sizes khác nhau\n",
    "                self.convs = nn.ModuleList([\n",
    "                    nn.Conv1d(\n",
    "                        in_channels=self.hidden_size,\n",
    "                        out_channels=cnn_num_filters,\n",
    "                        kernel_size=k,\n",
    "                        padding=k//2  # Same padding\n",
    "                    )\n",
    "                    for k in cnn_kernel_sizes\n",
    "                ])\n",
    "                \n",
    "                # Output size = số filters × số kernels\n",
    "                classifier_input_size = cnn_num_filters * len(cnn_kernel_sizes)\n",
    "                \n",
    "        else:\n",
    "            classifier_input_size = self.hidden_size\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Linear(classifier_input_size, num_labels)\n",
    "        \n",
    "        # Loss function\n",
    "        self.loss_fct = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask,\n",
    "        labels=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Returns:\n",
    "            dict với keys: loss (nếu labels được cung cấp), logits\n",
    "        \"\"\"\n",
    "        # BERT encoding\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            return_dict=True\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs.last_hidden_state  # (batch, seq_len, hidden)\n",
    "        \n",
    "        # Extra layers (nếu có)\n",
    "        if self.extra_layer is not None:\n",
    "            if self.extra_layer_type == 'lstm':\n",
    "                sequence_output, _ = self.extra_layer(sequence_output)\n",
    "                \n",
    "            elif self.extra_layer_type == 'cnn':\n",
    "                # CNN expects (batch, channels, seq_len)\n",
    "                # Input: (batch, seq_len, hidden) -> transpose to (batch, hidden, seq_len)\n",
    "                cnn_input = sequence_output.transpose(1, 2)\n",
    "                \n",
    "                # Apply multiple convolutions\n",
    "                conv_outputs = []\n",
    "                for conv in self.convs:\n",
    "                    # conv output: (batch, num_filters, seq_len)\n",
    "                    conv_out = F.relu(conv(cnn_input))\n",
    "                    conv_outputs.append(conv_out)\n",
    "                \n",
    "                # Concatenate along channel dimension\n",
    "                # (batch, num_filters * num_kernels, seq_len)\n",
    "                combined = torch.cat(conv_outputs, dim=1)\n",
    "                \n",
    "                # Transpose back: (batch, seq_len, num_filters * num_kernels)\n",
    "                sequence_output = combined.transpose(1, 2)\n",
    "        \n",
    "        # Dropout + Classification\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)  # (batch, seq_len, num_labels)\n",
    "        \n",
    "        # Calculate loss nếu có labels\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Flatten để tính loss\n",
    "            loss = self.loss_fct(\n",
    "                logits.view(-1, self.classifier.out_features),\n",
    "                labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,\n",
    "            'logits': logits\n",
    "        }\n",
    "\n",
    "logger.info(\"✅ Model class with CNN support defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f8277",
   "metadata": {
    "papermill": {
     "duration": 0.005854,
     "end_time": "2026-02-01T12:34:06.416359",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.410505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation & EvalHan2024-style Scorer\n",
    "- Stateful scorer: accumulate predictions qua batches\n",
    "- Bỏ qua padding và ignore labels theo đúng EvalHan2024\n",
    "- Per-label metrics + overall macro average\n",
    "- Pretty printing cho dễ đọc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f5444e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:06.429640Z",
     "iopub.status.busy": "2026-02-01T12:34:06.428964Z",
     "iopub.status.idle": "2026-02-01T12:34:06.434992Z",
     "shell.execute_reply": "2026-02-01T12:34:06.434497Z"
    },
    "papermill": {
     "duration": 0.014203,
     "end_time": "2026-02-01T12:34:06.436278",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.422075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dataclasses import asdict\n",
    "\n",
    "def save_eval_results(\n",
    "    results: dict,\n",
    "    task_config: TaskConfig,\n",
    "    training_config: TrainingConfig,\n",
    "    split: str,                       # \"val\" | \"test\"\n",
    "    output_dir: str = \"eval_results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Lưu kết quả đánh giá + siêu tham số ra file JSON (EvalHan2024 style)\n",
    "    \"\"\"\n",
    "    assert split in [\"val\", \"test\"], \"split must be 'val' or 'test'\"\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    model_tag = training_config.model_name.replace(\"/\", \"_\")\n",
    "\n",
    "    filename = (\n",
    "        f\"{task_config.task_name}_\"\n",
    "        f\"{split}_\"\n",
    "        f\"{model_tag}_\"\n",
    "        f\"{timestamp}.json\"\n",
    "    )\n",
    "\n",
    "    save_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    payload = {\n",
    "        \"meta\": {\n",
    "            \"task\": task_config.task_name,\n",
    "            \"split\": split,\n",
    "            \"timestamp\": timestamp\n",
    "        },\n",
    "        \"model\": {\n",
    "            \"name\": training_config.model_name,\n",
    "            \"num_labels\": task_config.num_labels,\n",
    "            \"labels\": task_config.labels,\n",
    "            \"ignore_labels\": task_config.ignore_labels\n",
    "        },\n",
    "        \"training_config\": asdict(training_config),\n",
    "        \"results\": results\n",
    "    }\n",
    "\n",
    "    with open(save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(payload, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    logger.info(f\"[INFO] Saved EvalHan results to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c7fd53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:06.449633Z",
     "iopub.status.busy": "2026-02-01T12:34:06.449386Z",
     "iopub.status.idle": "2026-02-01T12:34:06.464846Z",
     "shell.execute_reply": "2026-02-01T12:34:06.463910Z"
    },
    "papermill": {
     "duration": 0.023862,
     "end_time": "2026-02-01T12:34:06.466241",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.442379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from collections import defaultdict\n",
    "\n",
    "class EvalHanScorer:\n",
    "    \"\"\"\n",
    "    Scorer theo chuẩn EvalHan2024.\n",
    "    \n",
    "    Tính Precision/Recall/F1 cho:\n",
    "        - Từng loại label riêng biệt\n",
    "        - Overall (macro average)\n",
    "    \n",
    "    Bỏ qua:\n",
    "        - Padding tokens (label = -100)\n",
    "        - Labels trong ignore_labels\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: TaskConfig):\n",
    "        self.config = config\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset statistics\"\"\"\n",
    "        self.all_predictions = []\n",
    "        self.all_labels = []\n",
    "    \n",
    "    def add_batch(self, predictions, labels):\n",
    "        \"\"\"\n",
    "        Thêm một batch predictions và labels.\n",
    "        \n",
    "        Args:\n",
    "            predictions: tensor (batch, seq_len) - predicted label IDs\n",
    "            labels: tensor (batch, seq_len) - ground truth label IDs\n",
    "        \"\"\"\n",
    "        # Flatten và filter\n",
    "        predictions = predictions.view(-1).cpu().numpy()\n",
    "        labels = labels.view(-1).cpu().numpy()\n",
    "        \n",
    "        # Lọc padding (-100) và ignore labels\n",
    "        valid_mask = labels != -100\n",
    "        \n",
    "        for pred, label in zip(predictions[valid_mask], labels[valid_mask]):\n",
    "            label_str = self.config.id2label[label]\n",
    "            # Bỏ qua ignore labels (ví dụ: 'O' trong punctuation)\n",
    "            if label_str not in self.config.ignore_labels:\n",
    "                self.all_predictions.append(pred)\n",
    "                self.all_labels.append(label)\n",
    "    \n",
    "    def compute(self):\n",
    "        \"\"\"\n",
    "        Tính metrics theo chuẩn EvalHan2024.\n",
    "        \n",
    "        Returns:\n",
    "            dict với structure:\n",
    "                {\n",
    "                    'per_label': {\n",
    "                        'label_name': {'precision': ..., 'recall': ..., 'f1': ...}\n",
    "                    },\n",
    "                    'overall': {'precision': ..., 'recall': ..., 'f1': ...}\n",
    "                }\n",
    "        \"\"\"\n",
    "        if len(self.all_predictions) == 0:\n",
    "            return {'overall': {'precision': 0, 'recall': 0, 'f1': 0}, 'per_label': {}}\n",
    "        \n",
    "        # Get unique labels (exclude ignore labels)\n",
    "        unique_labels = []\n",
    "        for label_str in self.config.labels:\n",
    "            if label_str not in self.config.ignore_labels:\n",
    "                unique_labels.append(self.config.label2id[label_str])\n",
    "        \n",
    "        # Tính metrics cho từng label\n",
    "        precision, recall, f1, support = precision_recall_fscore_support(\n",
    "            self.all_labels,\n",
    "            self.all_predictions,\n",
    "            labels=unique_labels,\n",
    "            average=None,\n",
    "            zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Format results\n",
    "        results = {'per_label': {}}\n",
    "        \n",
    "        for idx, label_id in enumerate(unique_labels):\n",
    "            label_name = self.config.id2label[label_id]\n",
    "            results['per_label'][label_name] = {\n",
    "                'precision': float(precision[idx]),\n",
    "                'recall': float(recall[idx]),\n",
    "                'f1': float(f1[idx]),\n",
    "                'support': int(support[idx])\n",
    "            }\n",
    "        \n",
    "        # Overall metrics (macro average)\n",
    "        results['overall'] = {\n",
    "            'precision': float(np.mean(precision)),\n",
    "            'recall': float(np.mean(recall)),\n",
    "            'f1': float(np.mean(f1))\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results(self, results):\n",
    "        \"\"\"Pretty print results\"\"\"\n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\"EvalHan2024 Results - {self.config.task_name.upper()}\")\n",
    "        logger.info(f\"{'='*70}\")\n",
    "        \n",
    "        # Per-label results\n",
    "        logger.info(f\"\\n{'Label':<10} {'Precision':<12} {'Recall':<12} {'F1':<12} {'Support':<10}\")\n",
    "        logger.info(f\"{'-'*70}\")\n",
    "        \n",
    "        for label_name, metrics in results['per_label'].items():\n",
    "            logger.info(f\"{label_name:<10} \"\n",
    "                  f\"{metrics['precision']:<12.4f} \"\n",
    "                  f\"{metrics['recall']:<12.4f} \"\n",
    "                  f\"{metrics['f1']:<12.4f} \"\n",
    "                  f\"{metrics['support']:<10}\")\n",
    "        \n",
    "        # Overall results\n",
    "        logger.info(f\"{'-'*70}\")\n",
    "        logger.info(f\"{'OVERALL':<10} \"\n",
    "              f\"{results['overall']['precision']:<12.4f} \"\n",
    "              f\"{results['overall']['recall']:<12.4f} \"\n",
    "              f\"{results['overall']['f1']:<12.4f}\")\n",
    "        logger.info(f\"{'='*70}\\n\")\n",
    "\n",
    "def evaluate_model(\n",
    "    model,\n",
    "    dataloader,\n",
    "    config: TaskConfig,\n",
    "    device,\n",
    "    split: str,\n",
    "    training_config: TrainingConfig,\n",
    "    output_dir: str = \"eval_results\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate model trên validation/test set.\n",
    "    \n",
    "    Returns:\n",
    "        dict: EvalHan2024-style results\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    scorer = EvalHanScorer(config)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "            \n",
    "            # Add to scorer\n",
    "            scorer.add_batch(predictions, labels)\n",
    "    \n",
    "    # Compute metrics\n",
    "    results = scorer.compute()\n",
    "    scorer.print_results(results)\n",
    "\n",
    "    save_eval_results(\n",
    "        results=results,\n",
    "        task_config=config,\n",
    "        training_config=training_config,\n",
    "        split=split,\n",
    "        output_dir=output_dir\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "logger.info(\"✅ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425265a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:06.480433Z",
     "iopub.status.busy": "2026-02-01T12:34:06.480093Z",
     "iopub.status.idle": "2026-02-01T12:34:06.515864Z",
     "shell.execute_reply": "2026-02-01T12:34:06.515145Z"
    },
    "papermill": {
     "duration": 0.044627,
     "end_time": "2026-02-01T12:34:06.517275",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.472648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "========================================================================\n",
    "DATA LOADING UTILITIES\n",
    "========================================================================\n",
    "Hỗ trợ nhiều format phổ biến cho Classical Chinese data:\n",
    "- JSON format\n",
    "- CoNLL format (IOB style)\n",
    "- Plain text with inline labels\n",
    "- CSV format\n",
    "========================================================================\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import csv\n",
    "from typing import List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# ============================================================================\n",
    "# FORMAT 1: JSON Format\n",
    "# ============================================================================\n",
    "\n",
    "def load_json_format(file_path: str) -> Tuple[List[str], List[List[str]]]:\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        first = f.read(1)\n",
    "        f.seek(0)\n",
    "\n",
    "        # Case 1: JSON array\n",
    "        if first == \"[\":\n",
    "            data = json.load(f)\n",
    "            for item in data:\n",
    "                texts.append(item[\"text\"])\n",
    "                labels.append(item[\"labels\"])\n",
    "\n",
    "        # Case 2: JSONL\n",
    "        else:\n",
    "            for line in f:\n",
    "                if not line.strip():\n",
    "                    continue\n",
    "                item = json.loads(line)\n",
    "                texts.append(item[\"text\"])\n",
    "                labels.append(item[\"labels\"])\n",
    "\n",
    "    logger.info(f\"✓ Loaded {len(texts)} samples from {file_path}\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def save_json_format(texts: List[str], labels: List[List[str]], output_path: str):\n",
    "    \"\"\"Save data to JSON format\"\"\"\n",
    "    data = [{'text': t, 'labels': l} for t, l in zip(texts, labels)]\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    logger.info(f\"✓ Saved {len(texts)} samples to {output_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FORMAT 2: CoNLL Format (Character-level)\n",
    "# ============================================================================\n",
    "\n",
    "def load_conll_format(file_path: str) -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Load data từ CoNLL-style format.\n",
    "    \n",
    "    Expected format (character per line, blank line separates samples):\n",
    "    天 O\n",
    "    地 O\n",
    "    玄 O\n",
    "    黃 ，\n",
    "    \n",
    "    宇 O\n",
    "    宙 O\n",
    "    ...\n",
    "    \n",
    "    Returns:\n",
    "        texts: List[str]\n",
    "        labels: List[List[str]]\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    current_text = []\n",
    "    current_labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:  # Blank line = new sample\n",
    "                if current_text:\n",
    "                    texts.append(''.join(current_text))\n",
    "                    labels.append(current_labels)\n",
    "                    current_text = []\n",
    "                    current_labels = []\n",
    "            else:\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2:\n",
    "                    char, label = parts[0], parts[1]\n",
    "                    current_text.append(char)\n",
    "                    current_labels.append(label)\n",
    "        \n",
    "        # Don't forget last sample\n",
    "        if current_text:\n",
    "            texts.append(''.join(current_text))\n",
    "            labels.append(current_labels)\n",
    "    \n",
    "    logger.info(f\"✓ Loaded {len(texts)} samples from {file_path}\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "def save_conll_format(texts: List[str], labels: List[List[str]], output_path: str):\n",
    "    \"\"\"Save data to CoNLL format\"\"\"\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for text, label_seq in zip(texts, labels):\n",
    "            for char, label in zip(text, label_seq):\n",
    "                f.write(f\"{char} {label}\\n\")\n",
    "            f.write(\"\\n\")  # Blank line between samples\n",
    "    \n",
    "    logger.info(f\"✓ Saved {len(texts)} samples to {output_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FORMAT 3: Inline Format (text with embedded punctuation)\n",
    "# ============================================================================\n",
    "\n",
    "def load_inline_punctuation(file_path: str, \n",
    "                           punctuation_marks: List[str] = None) -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Load text với dấu câu inline, convert thành format chuẩn.\n",
    "    \n",
    "    Input: \"天地玄黃，宇宙洪荒。\"\n",
    "    Output: \n",
    "        text: \"天地玄黃宇宙洪荒\"\n",
    "        labels: ['O', 'O', 'O', 'O', '，', 'O', 'O', 'O', 'O', '。']\n",
    "    \n",
    "    Args:\n",
    "        file_path: path to file (one sample per line)\n",
    "        punctuation_marks: list of punctuation to extract\n",
    "    \n",
    "    Returns:\n",
    "        texts: List[str] (without punctuation)\n",
    "        labels: List[List[str]] (labels at character positions)\n",
    "    \"\"\"\n",
    "    if punctuation_marks is None:\n",
    "        punctuation_marks = ['，', '。', '：', '、', '；', '？', '！']\n",
    "    \n",
    "    punct_set = set(punctuation_marks)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            current_text = []\n",
    "            current_labels = []\n",
    "            \n",
    "            for i, char in enumerate(line):\n",
    "                if char in punct_set:\n",
    "                    # Dấu câu gán cho ký tự trước đó\n",
    "                    if current_labels:\n",
    "                        current_labels[-1] = char\n",
    "                else:\n",
    "                    current_text.append(char)\n",
    "                    current_labels.append('O')\n",
    "            \n",
    "            if current_text:\n",
    "                texts.append(''.join(current_text))\n",
    "                labels.append(current_labels)\n",
    "    \n",
    "    logger.info(f\"✓ Loaded {len(texts)} samples from {file_path}\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FORMAT 4: CSV Format\n",
    "# ============================================================================\n",
    "\n",
    "def load_csv_format(file_path: str, \n",
    "                   text_column: str = 'text',\n",
    "                   label_column: str = 'labels',\n",
    "                   delimiter: str = ',') -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Load data từ CSV.\n",
    "    \n",
    "    Expected CSV columns:\n",
    "        text,labels\n",
    "        \"天地玄黃\",\"O O O ，\"\n",
    "    \n",
    "    Args:\n",
    "        file_path: path to CSV\n",
    "        text_column: name of text column\n",
    "        label_column: name of labels column\n",
    "        delimiter: CSV delimiter\n",
    "    \n",
    "    Returns:\n",
    "        texts: List[str]\n",
    "        labels: List[List[str]]\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter=delimiter)\n",
    "        for row in reader:\n",
    "            text = row[text_column]\n",
    "            label_str = row[label_column]\n",
    "            \n",
    "            # Parse labels (assume space-separated)\n",
    "            label_list = label_str.strip().split()\n",
    "            \n",
    "            texts.append(text)\n",
    "            labels.append(label_list)\n",
    "    \n",
    "    logger.info(f\"✓ Loaded {len(texts)} samples from {file_path}\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FORMAT 5: BEMS from sentence boundaries\n",
    "# ============================================================================\n",
    "\n",
    "def create_bems_labels_from_sentences(sentences: List[str]) -> Tuple[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Convert list of sentences thành text + BEMS labels.\n",
    "    \n",
    "    Input: [\"天地玄黃\", \"宇宙洪荒\"]\n",
    "    Output:\n",
    "        text: \"天地玄黃宇宙洪荒\"\n",
    "        labels: ['B','M','M','E','B','M','M','E']\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of sentences\n",
    "    \n",
    "    Returns:\n",
    "        text: concatenated text\n",
    "        labels: BEMS labels\n",
    "    \"\"\"\n",
    "    text = ''.join(sentences)\n",
    "    labels = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        length = len(sentence)\n",
    "        if length == 1:\n",
    "            labels.append('S')\n",
    "        else:\n",
    "            labels.append('B')\n",
    "            labels.extend(['M'] * (length - 2))\n",
    "            labels.append('E')\n",
    "    \n",
    "    return text, labels\n",
    "\n",
    "\n",
    "def load_sentence_file_to_bems(file_path: str, \n",
    "                               sentence_delimiter: str = '\\n') -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Load file với sentences (one per line hoặc separated by delimiter),\n",
    "    convert thành BEMS format.\n",
    "    \n",
    "    Input file:\n",
    "        天地玄黃\n",
    "        宇宙洪荒\n",
    "        \n",
    "        日月盈昃\n",
    "        辰宿列張\n",
    "    \n",
    "    (blank line separates documents)\n",
    "    \n",
    "    Returns:\n",
    "        texts: List[str]\n",
    "        labels: List[List[str]] (BEMS)\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    labels = []\n",
    "    current_sentences = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            \n",
    "            if not line:  # Blank line = new document\n",
    "                if current_sentences:\n",
    "                    text, label_seq = create_bems_labels_from_sentences(current_sentences)\n",
    "                    texts.append(text)\n",
    "                    labels.append(label_seq)\n",
    "                    current_sentences = []\n",
    "            else:\n",
    "                current_sentences.append(line)\n",
    "        \n",
    "        # Last document\n",
    "        if current_sentences:\n",
    "            text, label_seq = create_bems_labels_from_sentences(current_sentences)\n",
    "            texts.append(text)\n",
    "            labels.append(label_seq)\n",
    "    \n",
    "    logger.info(f\"✓ Loaded {len(texts)} samples from {file_path}\")\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# AUTO-DETECT FORMAT\n",
    "# ============================================================================\n",
    "\n",
    "def load_data_auto(file_path: str, **kwargs) -> Tuple[List[str], List[List[str]]]:\n",
    "    \"\"\"\n",
    "    Tự động detect format và load data.\n",
    "    \n",
    "    Args:\n",
    "        file_path: path to data file\n",
    "        **kwargs: additional arguments for specific loaders\n",
    "    \n",
    "    Returns:\n",
    "        texts: List[str]\n",
    "        labels: List[List[str]]\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    suffix = file_path.suffix.lower()\n",
    "    \n",
    "    logger.info(f\"Auto-detecting format for: {file_path}\")\n",
    "    \n",
    "    if suffix == '.json' or suffix == '.jsonl':\n",
    "        return load_json_format(file_path)\n",
    "    elif suffix == '.csv':\n",
    "        return load_csv_format(file_path, **kwargs)\n",
    "    elif suffix in ['.txt', '.conll']:\n",
    "        # Try to detect: CoNLL vs inline vs sentence format\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            first_line = f.readline().strip()\n",
    "        \n",
    "        if '\\t' in first_line or (len(first_line.split()) == 2):\n",
    "            logger.info(\"  Detected: CoNLL format\")\n",
    "            return load_conll_format(file_path)\n",
    "        else:\n",
    "            logger.info(\"  Detected: Plain text format\")\n",
    "            logger.info(\"  Assuming inline punctuation - specify format if incorrect\")\n",
    "            return load_inline_punctuation(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format: {suffix}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def validate_data(texts: List[str], labels: List[List[str]]) -> bool:\n",
    "    \"\"\"\n",
    "    Validate data integrity.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if valid, raises exception otherwise\n",
    "    \"\"\"\n",
    "    assert len(texts) == len(labels), \\\n",
    "        f\"Length mismatch: {len(texts)} texts vs {len(labels)} label sequences\"\n",
    "    \n",
    "    for i, (text, label_seq) in enumerate(zip(texts, labels)):\n",
    "        assert len(text) == len(label_seq), \\\n",
    "            f\"Sample {i}: {len(text)} chars vs {len(label_seq)} labels\\n\" \\\n",
    "            f\"  Text: {text[:50]}...\\n\" \\\n",
    "            f\"  Labels: {label_seq[:50]}...\"\n",
    "    \n",
    "    logger.info(f\"✓ Data validation passed: {len(texts)} samples\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def print_data_stats(texts: List[str], labels: List[List[str]], task_config):\n",
    "    \"\"\"Print statistics about dataset\"\"\"\n",
    "    from collections import Counter\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(\"DATA STATISTICS\")\n",
    "    logger.info(f\"{'='*70}\")\n",
    "    logger.info(f\"Total samples: {len(texts)}\")\n",
    "    logger.info(f\"Avg text length: {sum(len(t) for t in texts) / len(texts):.1f} chars\")\n",
    "    logger.info(f\"Min/Max length: {min(len(t) for t in texts)} / {max(len(t) for t in texts)}\")\n",
    "    \n",
    "    # Label distribution\n",
    "    all_labels = [label for label_seq in labels for label in label_seq]\n",
    "    label_counts = Counter(all_labels)\n",
    "    \n",
    "    logger.info(f\"\\nLabel distribution:\")\n",
    "    for label in task_config.labels:\n",
    "        count = label_counts.get(label, 0)\n",
    "        pct = 100 * count / len(all_labels) if all_labels else 0\n",
    "        logger.info(f\"  {label}: {count:>8} ({pct:>5.2f}%)\")\n",
    "    \n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Example usage:\n",
    "\n",
    "# Auto-detect format\n",
    "texts, labels = load_data_auto('/kaggle/input/mydata/train.json')\n",
    "\n",
    "# Specific format\n",
    "texts, labels = load_json_format('/kaggle/input/mydata/train.json')\n",
    "texts, labels = load_conll_format('/kaggle/input/mydata/train.conll')\n",
    "\n",
    "# Validate\n",
    "validate_data(texts, labels)\n",
    "print_data_stats(texts, labels, PUNCTUATION_CONFIG)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937d815",
   "metadata": {
    "papermill": {
     "duration": 0.006254,
     "end_time": "2026-02-01T12:34:06.530157",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.523903",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Procedure\n",
    "- Gradient clipping để stability\n",
    "- Learning rate warmup (quan trọng cho BERT-based models)\n",
    "- Early stopping để tránh overfitting\n",
    "- Checkpoint best model theo F1 score\n",
    "- Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a23833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:06.544439Z",
     "iopub.status.busy": "2026-02-01T12:34:06.543719Z",
     "iopub.status.idle": "2026-02-01T12:34:06.549900Z",
     "shell.execute_reply": "2026-02-01T12:34:06.549342Z"
    },
    "papermill": {
     "duration": 0.01495,
     "end_time": "2026-02-01T12:34:06.551299",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.536349",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping handler\"\"\"\n",
    "    \n",
    "    def __init__(self, patience: int = 3, min_delta: float = 0.0, mode: str = 'max'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience: Number of epochs to wait for improvement\n",
    "            min_delta: Minimum change to qualify as improvement\n",
    "            mode: 'max' for metrics to maximize (F1), 'min' for loss\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.best_epoch = 0\n",
    "    \n",
    "    def __call__(self, score: float, epoch: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if should stop training.\n",
    "        \n",
    "        Returns:\n",
    "            True if should stop, False otherwise\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            return False\n",
    "        \n",
    "        if self.mode == 'max':\n",
    "            improved = score > (self.best_score + self.min_delta)\n",
    "        else:\n",
    "            improved = score < (self.best_score - self.min_delta)\n",
    "        \n",
    "        if improved:\n",
    "            self.best_score = score\n",
    "            self.best_epoch = epoch\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                return True\n",
    "        \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced05f32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:06.565337Z",
     "iopub.status.busy": "2026-02-01T12:34:06.565056Z",
     "iopub.status.idle": "2026-02-01T12:34:08.478112Z",
     "shell.execute_reply": "2026-02-01T12:34:08.477410Z"
    },
    "papermill": {
     "duration": 1.921993,
     "end_time": "2026-02-01T12:34:08.479781",
     "exception": false,
     "start_time": "2026-02-01T12:34:06.557788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from typing import Optional\n",
    "import optuna\n",
    "\n",
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    task_config: TaskConfig,\n",
    "    training_config: TrainingConfig,\n",
    "    trial: Optional[optuna.Trial] = None,\n",
    "    save_path: str = \"models/best_model_cnn.pt\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train model with early stopping.\n",
    "    \n",
    "    Args:\n",
    "        trial: Optuna trial (for pruning)\n",
    "    \n",
    "    Returns:\n",
    "        best_val_f1: Best validation F1 score\n",
    "    \"\"\"\n",
    "    set_seed(training_config.seed)\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = AdamW(\n",
    "        model.parameters(),\n",
    "        lr=training_config.learning_rate,\n",
    "        weight_decay=training_config.weight_decay\n",
    "    )\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * training_config.num_epochs\n",
    "    warmup_steps = int(total_steps * training_config.warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stopping = EarlyStopping(\n",
    "        patience=training_config.early_stopping_patience,\n",
    "        mode='max'\n",
    "    )\n",
    "    \n",
    "    best_val_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(training_config.num_epochs):\n",
    "        # ====================================================================\n",
    "        # TRAINING\n",
    "        # ====================================================================\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{training_config.num_epochs}\")\n",
    "        for step, batch in enumerate(progress_bar):\n",
    "            input_ids = batch['input_ids'].to(training_config.device)\n",
    "            attention_mask = batch['attention_mask'].to(training_config.device)\n",
    "            labels = batch['labels'].to(training_config.device)\n",
    "            \n",
    "            # Forward\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs['loss']\n",
    "            loss = loss.mean()\n",
    "            \n",
    "            # Backward\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), training_config.max_grad_norm)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            loss_value = loss.item()\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "            # Train log\n",
    "            \n",
    "            logger.info(\n",
    "                \"Epoch %d | Step %d/%d | Loss %.4f\",\n",
    "                epoch + 1, step, len(train_loader), loss_value\n",
    "            )\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # VALIDATION\n",
    "        # ====================================================================\n",
    "        val_results = evaluate_model(model, val_loader, task_config, training_config.device, \"test\", training_config)\n",
    "        val_f1 = val_results['overall']['f1']\n",
    "        \n",
    "        logger.info(f\"\\nEpoch {epoch+1}/{training_config.num_epochs}:\")\n",
    "        logger.info(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        logger.info(f\"  Val F1:     {val_f1:.4f}\")\n",
    "        logger.info(f\"  Val Prec:   {val_results['overall']['precision']:.4f}\")\n",
    "        logger.info(f\"  Val Recall: {val_results['overall']['recall']:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            logger.info(f\"  ✓ Saved best model (F1: {best_val_f1:.4f})\")\n",
    "        \n",
    "        # Optuna pruning (optional)\n",
    "        if trial is not None:\n",
    "            trial.report(val_f1, epoch)\n",
    "            if trial.should_prune():\n",
    "                logger.info(f\"  ✂️ Trial pruned at epoch {epoch+1}\")\n",
    "                raise optuna.TrialPruned()\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(val_f1, epoch):\n",
    "            logger.info(f\"\\n⏹️  Early stopping triggered!\")\n",
    "            logger.info(f\"  No improvement for {early_stopping.patience} epochs\")\n",
    "            logger.info(f\"  Best epoch: {early_stopping.best_epoch + 1}\")\n",
    "            logger.info(f\"  Best Val F1: {best_val_f1:.4f}\")\n",
    "            break\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(save_path))\n",
    "    \n",
    "    return best_val_f1\n",
    "\n",
    "logger.info(\"✅ Training functions with early stopping defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9135c",
   "metadata": {
    "papermill": {
     "duration": 0.006578,
     "end_time": "2026-02-01T12:34:08.493337",
     "exception": false,
     "start_time": "2026-02-01T12:34:08.486759",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# OPTUNA BAYESIAN OPTIMIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6119feef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:08.507902Z",
     "iopub.status.busy": "2026-02-01T12:34:08.507478Z",
     "iopub.status.idle": "2026-02-01T12:34:08.530273Z",
     "shell.execute_reply": "2026-02-01T12:34:08.529524Z"
    },
    "papermill": {
     "duration": 0.032038,
     "end_time": "2026-02-01T12:34:08.531789",
     "exception": false,
     "start_time": "2026-02-01T12:34:08.499751",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "from optuna.importance import MeanDecreaseImpurityImportanceEvaluator\n",
    "\n",
    "def create_optuna_objective(\n",
    "    train_texts, train_labels,\n",
    "    val_texts, val_labels,\n",
    "    tokenizer,\n",
    "    task_config: TaskConfig,\n",
    "    base_training_config: TrainingConfig\n",
    "):\n",
    "    \"\"\"\n",
    "    Create Optuna objective function.\n",
    "    \n",
    "    Returns a function that Optuna will optimize.\n",
    "    \"\"\"\n",
    "    \n",
    "    def objective(trial: optuna.Trial):\n",
    "        \"\"\"\n",
    "        Objective function for Optuna to maximize.\n",
    "        \n",
    "        Samples hyperparameters and returns validation F1.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ====================================================================\n",
    "        # SAMPLE HYPERPARAMETERS\n",
    "        # ====================================================================\n",
    "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True)\n",
    "        # batch_size = trial.suggest_categorical('batch_size', [8, 16, 32, 64])\n",
    "        batch_size = trial.suggest_categorical('batch_size', [64])\n",
    "        warmup_ratio = trial.suggest_float('warmup_ratio', 0.0, 0.2)\n",
    "        weight_decay = trial.suggest_float('weight_decay', 0.0, 0.1)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.3)\n",
    "        \n",
    "        logger.info(f\"\\n{'='*70}\")\n",
    "        logger.info(f\"Trial {trial.number}\")\n",
    "        logger.info(f\"{'='*70}\")\n",
    "        logger.info(f\"Hyperparameters:\")\n",
    "        logger.info(f\"  learning_rate: {learning_rate:.2e}\")\n",
    "        logger.info(f\"  batch_size:    {batch_size}\")\n",
    "        logger.info(f\"  warmup_ratio:  {warmup_ratio:.3f}\")\n",
    "        logger.info(f\"  weight_decay:  {weight_decay:.3f}\")\n",
    "        logger.info(f\"  dropout:       {dropout:.3f}\")\n",
    "        logger.info(f\"{'='*70}\")\n",
    "        \n",
    "        # ====================================================================\n",
    "        # CREATE DATALOADERS\n",
    "        # ====================================================================\n",
    "        train_dataset = ClassicalChineseDataset(\n",
    "            train_texts, train_labels, tokenizer, task_config, base_training_config.max_length\n",
    "        )\n",
    "        val_dataset = ClassicalChineseDataset(\n",
    "            val_texts, val_labels, tokenizer, task_config, base_training_config.max_length\n",
    "        )\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=1,\n",
    "            pin_memory=True,\n",
    "            persistent_workers=True\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # CREATE MODEL\n",
    "        # ====================================================================\n",
    "        model = SikuBERTForTokenClassification(\n",
    "            model_name=base_training_config.model_name,\n",
    "            num_labels=task_config.num_labels,\n",
    "            dropout=dropout,\n",
    "            use_extra_layer=True,\n",
    "            extra_layer_type='cnn',\n",
    "            cnn_kernel_sizes=[3, 5, 7], # Custom kernels\n",
    "            cnn_num_filters=256 # Nhiều filters hơn\n",
    "        ).to(base_training_config.device)\n",
    "        \n",
    "        # ====================================================================\n",
    "        # CREATE TRAINING CONFIG\n",
    "        # ====================================================================\n",
    "        trial_config = TrainingConfig(\n",
    "            model_name=base_training_config.model_name,\n",
    "            max_length=base_training_config.max_length,\n",
    "            learning_rate=learning_rate,\n",
    "            batch_size=batch_size,\n",
    "            num_epochs=base_training_config.num_epochs,\n",
    "            warmup_ratio=warmup_ratio,\n",
    "            weight_decay=weight_decay,\n",
    "            dropout=dropout,\n",
    "            early_stopping_patience=base_training_config.early_stopping_patience,\n",
    "            device=base_training_config.device,\n",
    "            seed=base_training_config.seed\n",
    "        )\n",
    "        \n",
    "        # ====================================================================\n",
    "        # TRAIN WITH EARLY STOPPING\n",
    "        # ====================================================================\n",
    "        try:\n",
    "            best_val_f1 = train_with_early_stopping(\n",
    "                model, train_loader, val_loader,\n",
    "                task_config, trial_config,\n",
    "                trial=trial,\n",
    "                save_path=f\"models/optuna_trial_{trial.number}_best_cnn.pt\"\n",
    "            )\n",
    "            \n",
    "            logger.info(f\"\\n✓ Trial {trial.number} completed: Val F1 = {best_val_f1:.4f}\")\n",
    "            \n",
    "            return best_val_f1\n",
    "            \n",
    "        except optuna.TrialPruned:\n",
    "            # Trial was pruned by Optuna\n",
    "            raise\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.info(f\"\\n❌ Trial {trial.number} failed: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    return objective\n",
    "\n",
    "\n",
    "def run_optuna_optimization(\n",
    "    train_texts, train_labels,\n",
    "    val_texts, val_labels,\n",
    "    tokenizer,\n",
    "    task_config: TaskConfig,\n",
    "    base_training_config: TrainingConfig,\n",
    "    n_trials: int = 30,\n",
    "    study_name: str = \"sikubert_tuning\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Run Optuna hyperparameter optimization.\n",
    "    \n",
    "    Args:\n",
    "        n_trials: Number of trials to run\n",
    "        study_name: Name of the study\n",
    "    \n",
    "    Returns:\n",
    "        study: Optuna study object with all results\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\"OPTUNA BAYESIAN OPTIMIZATION\")\n",
    "    logger.info(f\"{'='*70}\")\n",
    "    logger.info(f\"Task: {task_config.task_name}\")\n",
    "    logger.info(f\"Number of trials: {n_trials}\")\n",
    "    logger.info(f\"Early stopping patience: {base_training_config.early_stopping_patience}\")\n",
    "    logger.info(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Create objective function\n",
    "    objective = create_optuna_objective(\n",
    "        train_texts, train_labels,\n",
    "        val_texts, val_labels,\n",
    "        tokenizer, task_config, base_training_config\n",
    "    )\n",
    "    \n",
    "    # Create study\n",
    "    study = optuna.create_study(\n",
    "        study_name=study_name,\n",
    "        direction='maximize',  # Maximize F1\n",
    "        pruner=optuna.pruners.MedianPruner(  # Prune unpromising trials\n",
    "            n_startup_trials=5,\n",
    "            n_warmup_steps=3\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Run optimization\n",
    "    study.optimize(\n",
    "        objective,\n",
    "        n_trials=n_trials,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    \n",
    "    # ========================================================================\n",
    "    # RESULTS\n",
    "    # ========================================================================\n",
    "    logger.info(f\"\\n{'='*70}\")\n",
    "    logger.info(f\"OPTIMIZATION COMPLETE\")\n",
    "    logger.info(f\"{'='*70}\")\n",
    "    \n",
    "    logger.info(f\"\\n📊 Study Statistics:\")\n",
    "    logger.info(f\"  Completed trials: {len(study.trials)}\")\n",
    "    logger.info(f\"  Pruned trials:    {len([t for t in study.trials if t.state == optuna.trial.TrialState.PRUNED])}\")\n",
    "    logger.info(f\"  Failed trials:    {len([t for t in study.trials if t.state == optuna.trial.TrialState.FAIL])}\")\n",
    "    \n",
    "    logger.info(f\"\\n🏆 Best Trial:\")\n",
    "    best_trial = study.best_trial\n",
    "    logger.info(f\"  Trial number:  {best_trial.number}\")\n",
    "    logger.info(f\"  Val F1:        {best_trial.value:.4f}\")\n",
    "    logger.info(f\"\\n  Best Hyperparameters:\")\n",
    "    for key, value in best_trial.params.items():\n",
    "        if 'learning_rate' in key:\n",
    "            logger.info(f\"    {key}: {value:.2e}\")\n",
    "        else:\n",
    "            logger.info(f\"    {key}: {value}\")\n",
    "    \n",
    "    # Save results\n",
    "    results = {\n",
    "        'study_name': study_name,\n",
    "        'task': task_config.task_name,\n",
    "        'n_trials': len(study.trials),\n",
    "        'best_trial': {\n",
    "            'number': best_trial.number,\n",
    "            'value': best_trial.value,\n",
    "            'params': best_trial.params\n",
    "        },\n",
    "        'all_trials': [\n",
    "            {\n",
    "                'number': t.number,\n",
    "                'value': t.value,\n",
    "                'params': t.params,\n",
    "                'state': str(t.state)\n",
    "            }\n",
    "            for t in study.trials\n",
    "        ],\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(f'outputs/optuna_{task_config.task_name}_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"\\n✓ Results saved to outputs/optuna_{task_config.task_name}_results.json\")\n",
    "    \n",
    "    return study\n",
    "\n",
    "logger.info(\"✅ Optuna optimization functions defined\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def visualize_optuna_results(study, task_name: str):\n",
    "    \"\"\"Visualize Optuna results\"\"\"\n",
    "    try:\n",
    "        # Plot 1: Optimization history\n",
    "        fig1 = plot_optimization_history(study)\n",
    "        fig1.write_html(f'outputs/optuna_{task_name}_history.html')\n",
    "        logger.info(f\"✓ Saved optimization history plot\")\n",
    "        \n",
    "        # Plot 2: Parameter importances\n",
    "        # FIX: Use the Random Forest evaluator to avoid the NumPy/fANOVA ValueError\n",
    "        fig2 = plot_param_importances(\n",
    "            study, \n",
    "            evaluator=MeanDecreaseImpurityImportanceEvaluator()\n",
    "        )\n",
    "        \n",
    "        # FIX: Save as HTML to avoid needing the 'kaleido' package\n",
    "        fig2.write_html(f'outputs/optuna_{task_name}_importance.html')\n",
    "        logger.info(f\"✓ Saved parameter importance plot\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        logger.info(f\"⚠️ Visualization failed: {e}\")\n",
    "        logger.info(\"   Ensure 'plotly' is installed.\")\n",
    "    except Exception as e:\n",
    "        logger.info(f\"⚠️ An unexpected error occurred during visualization: {e}\")\n",
    "\n",
    "logger.info(\"✅ Visualization functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb796761",
   "metadata": {
    "papermill": {
     "duration": 0.006457,
     "end_time": "2026-02-01T12:34:08.545219",
     "exception": false,
     "start_time": "2026-02-01T12:34:08.538762",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78897cb5",
   "metadata": {
    "papermill": {
     "duration": 0.006311,
     "end_time": "2026-02-01T12:34:08.557790",
     "exception": false,
     "start_time": "2026-02-01T12:34:08.551479",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "##  Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a256d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:08.571119Z",
     "iopub.status.busy": "2026-02-01T12:34:08.570642Z",
     "iopub.status.idle": "2026-02-01T12:34:09.491663Z",
     "shell.execute_reply": "2026-02-01T12:34:09.490903Z"
    },
    "papermill": {
     "duration": 0.929857,
     "end_time": "2026-02-01T12:34:09.493634",
     "exception": false,
     "start_time": "2026-02-01T12:34:08.563777",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_config = TrainingConfig(early_stopping_patience=3)\n",
    "\n",
    "# 2. Setup\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_config.model_name)\n",
    "logger.info(\"✓ Tokenizer loaded\")\n",
    "\n",
    "logger.info(f\"\\n{'='*70}\")\n",
    "logger.info(f\"Training: {task_config.task_name.upper()}\")\n",
    "logger.info(f\"Labels: {task_config.labels}\")\n",
    "logger.info(f\"Device: {base_config.device}\")\n",
    "logger.info(f\"{'='*70}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00a6f3",
   "metadata": {
    "papermill": {
     "duration": 0.006921,
     "end_time": "2026-02-01T12:34:09.509502",
     "exception": false,
     "start_time": "2026-02-01T12:34:09.502581",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27b6cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:34:09.526187Z",
     "iopub.status.busy": "2026-02-01T12:34:09.525643Z",
     "iopub.status.idle": "2026-02-01T12:35:35.834923Z",
     "shell.execute_reply": "2026-02-01T12:35:35.834014Z"
    },
    "papermill": {
     "duration": 86.330305,
     "end_time": "2026-02-01T12:35:35.847142",
     "exception": false,
     "start_time": "2026-02-01T12:34:09.516837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your data\n",
    "train_texts, train_labels = load_data_auto(train_path)\n",
    "val_texts, val_labels = load_data_auto(val_path)\n",
    "\n",
    "# train_texts=train_texts[:5]\n",
    "# train_labels=train_labels[:5]\n",
    "# val_texts=val_texts[:5]\n",
    "# val_labels=val_labels[:5]\n",
    "\n",
    "# Validate data\n",
    "logger.info(\"\\n📊 Validating data...\")\n",
    "validate_data(train_texts, train_labels)\n",
    "validate_data(val_texts, val_labels)\n",
    "\n",
    "logger.info(f\"\\n✓ Data loaded successfully:\")\n",
    "logger.info(f\"  - Training set: {len(train_texts)} samples\")\n",
    "logger.info(f\"  - Validation set: {len(val_texts)} samples\")\n",
    "logger.info(f\"  - Train/Val ratio: {len(train_texts)/len(val_texts):.1f}:1\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    train_texts, train_labels, val_texts, val_labels,\n",
    "    tokenizer, task_config, training_config=base_config\n",
    ")\n",
    "\n",
    "print_data_stats(train_texts, train_texts, PUNCTUATION_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be124804",
   "metadata": {
    "papermill": {
     "duration": 0.008819,
     "end_time": "2026-02-01T12:35:35.866646",
     "exception": false,
     "start_time": "2026-02-01T12:35:35.857827",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Run Optuna to fine best Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dfd50f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:35:35.883300Z",
     "iopub.status.busy": "2026-02-01T12:35:35.882770Z",
     "iopub.status.idle": "2026-02-01T12:35:35.886129Z",
     "shell.execute_reply": "2026-02-01T12:35:35.885468Z"
    },
    "papermill": {
     "duration": 0.013388,
     "end_time": "2026-02-01T12:35:35.887570",
     "exception": false,
     "start_time": "2026-02-01T12:35:35.874182",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# study = run_optuna_optimization(\n",
    "#     train_texts, train_labels,\n",
    "#     val_texts, val_labels,\n",
    "#     tokenizer, task_config, base_config,\n",
    "#     n_trials=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8887192",
   "metadata": {
    "papermill": {
     "duration": 0.00722,
     "end_time": "2026-02-01T12:35:35.902308",
     "exception": false,
     "start_time": "2026-02-01T12:35:35.895088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Visualize Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a275883",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:35:35.918621Z",
     "iopub.status.busy": "2026-02-01T12:35:35.918411Z",
     "iopub.status.idle": "2026-02-01T12:35:35.921921Z",
     "shell.execute_reply": "2026-02-01T12:35:35.921368Z"
    },
    "papermill": {
     "duration": 0.013657,
     "end_time": "2026-02-01T12:35:35.923259",
     "exception": false,
     "start_time": "2026-02-01T12:35:35.909602",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualize_optuna_results(study, TASK)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149496aa",
   "metadata": {
    "papermill": {
     "duration": 0.007405,
     "end_time": "2026-02-01T12:35:35.938409",
     "exception": false,
     "start_time": "2026-02-01T12:35:35.931004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Train final model with best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04268fa6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:35:35.954226Z",
     "iopub.status.busy": "2026-02-01T12:35:35.953990Z",
     "iopub.status.idle": "2026-02-01T12:35:35.957379Z",
     "shell.execute_reply": "2026-02-01T12:35:35.956722Z"
    },
    "papermill": {
     "duration": 0.012988,
     "end_time": "2026-02-01T12:35:35.958755",
     "exception": false,
     "start_time": "2026-02-01T12:35:35.945767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# best_params = study.best_params\n",
    "# final_config = TrainingConfig(\n",
    "#     learning_rate=best_params['learning_rate'],\n",
    "#     batch_size=best_params['batch_size'],\n",
    "#     warmup_ratio=best_params['warmup_ratio'],\n",
    "#     weight_decay=best_params['weight_decay'],\n",
    "#     dropout=best_params['dropout']\n",
    "# )\n",
    "\n",
    "# model = SikuBERTForTokenClassification(\n",
    "#     final_config.model_name,\n",
    "#     task_config.num_labels,\n",
    "#     use_extra_layer=True,\n",
    "#     extra_layer_type='cnn',\n",
    "#     cnn_kernel_sizes=[3, 5, 7], # Custom kernels\n",
    "#     cnn_num_filters=256 # Nhiều filters hơn\n",
    "# ).to(final_config.device)\n",
    "\n",
    "# logger.info(f\"✓ Model created ({sum(p.numel() for p in model.parameters()):,} parameters)\")\n",
    "# save_path = f\"models/best_{task_config.task_name}_model_cnn.pt\"\n",
    "\n",
    "# best_val_f1 = train_with_early_stopping(\n",
    "#     model, train_loader, val_loader,\n",
    "#     task_config, final_config,\n",
    "#     trial=None,\n",
    "#     save_path=save_path\n",
    "# )\n",
    "\n",
    "# logger.info(\"\\n🎉 Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e559d5ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T12:35:35.975700Z",
     "iopub.status.busy": "2026-02-01T12:35:35.975457Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2026-02-01T12:35:35.966673",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training only\n",
    "final_config=base_config\n",
    "\n",
    "model = SikuBERTForTokenClassification(\n",
    "    base_config.model_name,\n",
    "    task_config.num_labels,\n",
    "    use_extra_layer=True,\n",
    "    extra_layer_type='cnn',\n",
    "    cnn_kernel_sizes=[3, 5, 7], # Custom kernels\n",
    "    cnn_num_filters=256 # Nhiều filters hơn\n",
    ").to(base_config.device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "model = model.to(base_config.device)\n",
    "\n",
    "logger.info(f\"✓ Model created ({sum(p.numel() for p in model.parameters()):,} parameters)\")\n",
    "save_path = f\"models/best_{task_config.task_name}_model_cnn.pt\"\n",
    "\n",
    "best_val_f1 = train_with_early_stopping(\n",
    "    model, train_loader, val_loader,\n",
    "    task_config, base_config,\n",
    "    trial=None,\n",
    "    save_path=save_path\n",
    ")\n",
    "\n",
    "logger.info(\"\\n🎉 Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe13afaa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Final Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccefe65",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.255630Z",
     "iopub.status.idle": "2026-02-01T12:30:17.255976Z",
     "shell.execute_reply": "2026-02-01T12:30:17.255863Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.255846Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"⭐ FINAL TEST SET EVALUATION\")\n",
    "logger.info(\"=\"*70)\n",
    "logger.info(\"This is the OFFICIAL performance evaluation\")\n",
    "logger.info(\"Model has NEVER seen test data during training!\")\n",
    "logger.info(\"=\"*70)\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load(save_path))\n",
    "model.eval()\n",
    "\n",
    "test_texts, test_labels = load_data_auto(test_path)\n",
    "\n",
    "# Create test loader\n",
    "test_dataset = ClassicalChineseDataset(\n",
    "    test_texts, test_labels, tokenizer, task_config, final_config.max_length\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=final_config.batch_size, \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "logger.info(\"\\n🎯 Evaluating on TEST set...\")\n",
    "test_results = evaluate_model(model, test_loader, task_config, final_config.device, \"test\", final_config)\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "final_results = {\n",
    "    'task': task_config.task_name,\n",
    "    'test': test_results['overall'],  # ← OFFICIAL RESULT\n",
    "    'test_per_label': test_results['per_label']\n",
    "}\n",
    "\n",
    "with open(f'outputs/{task_config.task_name}_test_results.json', 'w') as f:\n",
    "    json.dump(final_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "logger.info(\"\\n\" + \"=\"*70)\n",
    "logger.info(\"✅ OFFICIAL TEST RESULTS:\")\n",
    "logger.info(f\"   Precision: {test_results['overall']['precision']:.4f}\")\n",
    "logger.info(f\"   Recall:    {test_results['overall']['recall']:.4f}\")\n",
    "logger.info(f\"   F1 Score:  {test_results['overall']['f1']:.4f}\")\n",
    "logger.info(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dba820",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# INFERENCE & DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57db61bb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.257396Z",
     "iopub.status.idle": "2026-02-01T12:30:17.257840Z",
     "shell.execute_reply": "2026-02-01T12:30:17.257621Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.257596Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_text(model, text, tokenizer, config, device):\n",
    "    \"\"\"Predict labels for text\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    tokenized = tokenizer(list(text), is_split_into_words=True, \n",
    "                         return_tensors='pt', padding=True, truncation=True)\n",
    "    input_ids = tokenized['input_ids'].to(device)\n",
    "    attention_mask = tokenized['attention_mask'].to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        predictions = torch.argmax(outputs['logits'], dim=-1)\n",
    "    \n",
    "    word_ids = tokenized.word_ids(batch_index=0)\n",
    "    predicted_labels = []\n",
    "    \n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is not None:\n",
    "            pred_id = predictions[0][idx].item()\n",
    "            predicted_labels.append(config.id2label[pred_id])\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ced8750",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.258973Z",
     "iopub.status.idle": "2026-02-01T12:30:17.259245Z",
     "shell.execute_reply": "2026-02-01T12:30:17.259137Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.259121Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Demo\n",
    "#\"綱鑑會編卷五十三。文安劉德芳匡訏正崑山葉澐麁輯錄唐紀高宗皇帝諱治，字爲善，太宗第九子。初封𣈆珏後立爲皇太子在位三十四年崩壽五十六綱。\"\n",
    "#\"綱鑑會編卷五十三 | 文安劉德芳匡訏正崑山葉澐麁輯錄唐紀高宗皇帝諱治 | 字爲善 | 太宗第九子/初封𣈆珏後立爲皇太子在位三十四年崩壽五十六綱 | \"\n",
    "\n",
    "test_text = \"綱鑑會編卷五十三文安劉德芳匡訏正崑山葉澐麁輯錄唐紀高宗皇帝諱治字爲善太宗第九子初封𣈆珏後立爲皇太子在位三十四年崩壽五十六綱\"\n",
    "predicted = predict_text(model, test_text, tokenizer, task_config, final_config.device)\n",
    "\n",
    "logger.info(f\"\\n{'='*70}\")\n",
    "logger.info(\"DEMO INFERENCE\")\n",
    "logger.info(f\"{'='*70}\")\n",
    "logger.info(f\"\\nText: {test_text}\")\n",
    "logger.info(f\"Labels: {predicted}\")  # Show first 20\n",
    "\n",
    "if TASK == \"punctuation\":\n",
    "    result = ''.join([c if l == 'O' else c+l for c, l in zip(test_text, predicted)])\n",
    "    logger.info(f\"\\nPunctuated: {result}\")\n",
    "else:\n",
    "    sentences = []\n",
    "    current = []\n",
    "    for c, l in zip(test_text, predicted):\n",
    "        current.append(c)\n",
    "        if l in ['E', 'S']:\n",
    "            sentences.append(''.join(current))\n",
    "            current = []\n",
    "    if current:\n",
    "        sentences.append(''.join(current))\n",
    "    logger.info(f\"\\nSegmented: {' | '.join(sentences)}\")\n",
    "\n",
    "logger.info(f\"\\n{'='*70}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370c4d25",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.260058Z",
     "iopub.status.idle": "2026-02-01T12:30:17.260443Z",
     "shell.execute_reply": "2026-02-01T12:30:17.260278Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.260254Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_punctuation_labels(text, labels):\n",
    "    \"\"\"\n",
    "    text: str\n",
    "    labels: List[str] with punctuation symbols or 'O'\n",
    "    \"\"\"\n",
    "    output = []\n",
    "\n",
    "    for ch, label in zip(text, labels):\n",
    "        output.append(ch)\n",
    "        if label != \"O\":\n",
    "            output.append(label)\n",
    "\n",
    "    return \"\".join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31700f9a",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.261850Z",
     "iopub.status.idle": "2026-02-01T12:30:17.262241Z",
     "shell.execute_reply": "2026-02-01T12:30:17.262054Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.262028Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_segmentation_inline(text, labels, sep=\" | \"):\n",
    "    output = []\n",
    "\n",
    "    for ch, label in zip(text, labels):\n",
    "        output.append(ch)\n",
    "        if label in (\"E\", \"S\"):\n",
    "            output.append(sep)\n",
    "\n",
    "    return \"\".join(output).rstrip(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b0d7b",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.263634Z",
     "iopub.status.idle": "2026-02-01T12:30:17.264096Z",
     "shell.execute_reply": "2026-02-01T12:30:17.263926Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.263901Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_labels(model, text, tokenizer, config, device):\n",
    "    \"\"\"\n",
    "    Predict labels for ONE text (character-level)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    chars = list(text)\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        chars,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized[\"input_ids\"].to(device)\n",
    "    attention_mask = tokenized[\"attention_mask\"].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        preds = torch.argmax(outputs[\"logits\"], dim=-1)[0]\n",
    "\n",
    "    word_ids = tokenized.word_ids()\n",
    "    pred_labels = []\n",
    "\n",
    "    for idx, word_id in enumerate(word_ids):\n",
    "        if word_id is not None:\n",
    "            label_id = preds[idx].item()\n",
    "            pred_labels.append(config.id2label[label_id])\n",
    "\n",
    "    return pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f167c2ee",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.267772Z",
     "iopub.status.idle": "2026-02-01T12:30:17.268171Z",
     "shell.execute_reply": "2026-02-01T12:30:17.267983Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.267956Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_test_set(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    config,\n",
    "    device,\n",
    "    test_path,\n",
    "    output_path,\n",
    "):\n",
    "    import json\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    with open(test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        test_data = json.load(f)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for sample in tqdm(test_data):\n",
    "        text = sample[\"text\"]\n",
    "        gold_labels = sample[\"labels\"]\n",
    "\n",
    "        pred_labels = predict_labels(\n",
    "            model=model,\n",
    "            text=text,\n",
    "            tokenizer=tokenizer,\n",
    "            config=config,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # assert len(text) == len(gold_labels) == len(pred_labels)\n",
    "\n",
    "        if config.task_name == \"punctuation\":\n",
    "            gold_text = apply_punctuation_labels(text, gold_labels)\n",
    "            pred_text = apply_punctuation_labels(text, pred_labels)\n",
    "\n",
    "        elif config.task_name == \"segmentation\":\n",
    "            gold_text = apply_segmentation_inline(text, gold_labels)\n",
    "            pred_text = apply_segmentation_inline(text, pred_labels)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task: {config.task_name}\")\n",
    "\n",
    "        results.append({\n",
    "            \"text\": text,\n",
    "            \"gold_labels\": gold_labels,\n",
    "            \"pred_labels\": pred_labels,\n",
    "            \"gold_text_labeled\": gold_text,\n",
    "            \"pred_text_labeled\": pred_text,\n",
    "        })\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61af2ee3",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2026-02-01T12:30:17.269057Z",
     "iopub.status.idle": "2026-02-01T12:30:17.269422Z",
     "shell.execute_reply": "2026-02-01T12:30:17.269238Z",
     "shell.execute_reply.started": "2026-02-01T12:30:17.269213Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_test_set(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=task_config,\n",
    "    device=device,\n",
    "    test_path=test_path,\n",
    "    output_path=\"/kaggle/working/test_pred.json\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 295228767,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31236,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-02-01T12:33:42.371255",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
