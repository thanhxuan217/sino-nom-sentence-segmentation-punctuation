# SikuBERT SLURM Training - Complete Project Structure

## ðŸ“ Directory Tree

```
sikubert-slurm-training/
â”‚
â”œâ”€â”€ ðŸ“„ train.py                          # Main training script
â”œâ”€â”€ ðŸ“„ requirements.txt                  # Python dependencies
â”œâ”€â”€ ðŸ“„ README.md                         # Documentation
â”‚
â”œâ”€â”€ ðŸ”§ Configuration Files
â”‚   â”œâ”€â”€ config.sh                        # Centralized configuration
â”‚   â”œâ”€â”€ run_slurm.sh                     # SLURM script (standalone)
â”‚   â”œâ”€â”€ run_slurm_with_config.sh         # SLURM script (uses config.sh)
â”‚   â””â”€â”€ run_slurm_multigpu.sh            # Multi-GPU SLURM script
â”‚
â”œâ”€â”€ ðŸ› ï¸ Utility Scripts
â”‚   â”œâ”€â”€ setup.sh                         # Initial setup script
â”‚   â””â”€â”€ slurm_helper.sh                  # Job management helper
â”‚
â”œâ”€â”€ ðŸ“‚ data/                             # Your training data
â”‚   â”œâ”€â”€ segmentation_train.json          # Training set
â”‚   â”œâ”€â”€ segmentation_val.json            # Validation set
â”‚   â”œâ”€â”€ segmentation_test.json           # Test set
â”‚   â”œâ”€â”€ punctuation_train.json           # (Alternative task)
â”‚   â”œâ”€â”€ punctuation_val.json
â”‚   â””â”€â”€ punctuation_test.json
â”‚
â”œâ”€â”€ ðŸ“‚ models/                           # Saved model checkpoints
â”‚   â”œâ”€â”€ best_segmentation_model_cnn.pt   # Best segmentation model
â”‚   â””â”€â”€ best_punctuation_model_cnn.pt    # Best punctuation model
â”‚
â”œâ”€â”€ ðŸ“‚ outputs/                          # Training outputs
â”‚   â”œâ”€â”€ train_segmentation.log           # Training log
â”‚   â”œâ”€â”€ segmentation_results.json        # Test results
â”‚   â””â”€â”€ test_pred.json                   # Detailed predictions
â”‚
â””â”€â”€ ðŸ“‚ logs/                             # SLURM job logs
    â”œâ”€â”€ slurm_12345.out                  # Job stdout
    â””â”€â”€ slurm_12345.err                  # Job stderr
```

---

## ðŸ“Š Data File Structures

### 1. Input Data Format (JSON)

**File**: `data/segmentation_train.json`

```json
[
  {
    "text": "å¤©åœ°çŽ„é»ƒå®‡å®™æ´ªè’",
    "labels": ["B", "M", "M", "E", "B", "M", "M", "E"]
  },
  {
    "text": "æ—¥æœˆç›ˆæ˜ƒè¾°å®¿åˆ—å¼µ",
    "labels": ["B", "M", "M", "E", "B", "M", "M", "E"]
  },
  {
    "text": "å¯’ä¾†æš‘å¾€ç§‹æ”¶å†¬è—",
    "labels": ["B", "M", "M", "E", "B", "M", "M", "E"]
  }
]
```

**Schema**:
- `text` (string): Raw Classical Chinese text without spaces or punctuation
- `labels` (array of strings): Character-level labels
  - For **segmentation**: `B` (Begin), `M` (Middle), `E` (End), `S` (Single)
  - For **punctuation**: `O` (no punctuation), `ï¼Œ`, `ã€‚`, `ï¼š`, `ã€`, `ï¼›`, `ï¼Ÿ`, `ï¼`

**Requirements**:
- `len(text) == len(labels)` for each sample
- All characters must have corresponding labels
- UTF-8 encoding

---

### 2. Configuration File Structure

**File**: `config.sh`

```bash
# Data paths - YOU MUST MODIFY THESE
export TRAIN_PATH="/path/to/data/segmentation_train.json"
export VAL_PATH="/path/to/data/segmentation_val.json"
export TEST_PATH="/path/to/data/segmentation_test.json"

# Task selection
export TASK="segmentation"  # or "punctuation"

# Model configuration
export MODEL_NAME="SIKU-BERT/sikubert"
export MAX_LENGTH=256

# Training hyperparameters
export BATCH_SIZE=64
export LEARNING_RATE=2e-5
export NUM_EPOCHS=5
export WARMUP_RATIO=0.1
export WEIGHT_DECAY=0.01
export DROPOUT=0.1
export SEED=42

# CNN architecture
export CNN_KERNEL_SIZES="3 5 7"
export CNN_NUM_FILTERS=256

# Output directories
export OUTPUT_DIR="outputs"
export MODEL_SAVE_DIR="models"
export LOG_DIR="logs"
```

---

### 3. Model Output Structure

**File**: `models/best_segmentation_model_cnn.pt`

```
PyTorch State Dict (.pt file)
â”œâ”€â”€ bert.embeddings.word_embeddings.weight     [vocab_size, 768]
â”œâ”€â”€ bert.embeddings.position_embeddings.weight [512, 768]
â”œâ”€â”€ bert.encoder.layer.0.attention...          [various shapes]
â”œâ”€â”€ ...
â”œâ”€â”€ extra_layer.convs.0.weight                 [256, 768, 3]
â”œâ”€â”€ extra_layer.convs.1.weight                 [256, 768, 5]
â”œâ”€â”€ extra_layer.convs.2.weight                 [256, 768, 7]
â””â”€â”€ classifier.weight                          [num_labels, 768]
```

**Size**: ~400-500 MB (depends on configuration)

---

### 4. Training Results Structure

**File**: `outputs/segmentation_results.json`

```json
{
  "task": "segmentation",
  "test_metrics": {
    "loss": 0.1234,
    "precision": 0.9567,
    "recall": 0.9523,
    "f1": 0.9545
  },
  "config": {
    "task": "segmentation",
    "train_path": "/path/to/segmentation_train.json",
    "val_path": "/path/to/segmentation_val.json",
    "test_path": "/path/to/segmentation_test.json",
    "model_name": "SIKU-BERT/sikubert",
    "max_length": 256,
    "batch_size": 64,
    "learning_rate": 2e-5,
    "num_epochs": 5,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "dropout": 0.1,
    "seed": 42,
    "cnn_kernel_sizes": [3, 5, 7],
    "cnn_num_filters": 256,
    "output_dir": "outputs",
    "model_save_dir": "models"
  }
}
```

---

### 5. Training Log Structure

**File**: `outputs/train_segmentation.log`

```
2024-02-02 10:00:00 | ======================================================================
2024-02-02 10:00:00 | TRAINING CONFIGURATION
2024-02-02 10:00:00 | ======================================================================
2024-02-02 10:00:00 | task: segmentation
2024-02-02 10:00:00 | train_path: /path/to/segmentation_train.json
2024-02-02 10:00:00 | batch_size: 64
2024-02-02 10:00:00 | learning_rate: 2e-05
2024-02-02 10:00:00 | ======================================================================
2024-02-02 10:00:01 | 
2024-02-02 10:00:01 | âœ“ Device: cuda
2024-02-02 10:00:01 |   GPU: NVIDIA A100-SXM4-40GB
2024-02-02 10:00:02 | 
2024-02-02 10:00:02 | âœ“ Task: segmentation
2024-02-02 10:00:02 |   Labels: ['B', 'M', 'E', 'S']
2024-02-02 10:00:02 |   Num labels: 4
2024-02-02 10:00:03 | 
2024-02-02 10:00:03 | âœ“ Loading tokenizer...
2024-02-02 10:00:04 | âœ“ Loading data...
2024-02-02 10:00:04 |   Train samples: 10000
2024-02-02 10:00:04 |   Val samples: 2000
2024-02-02 10:00:05 | âœ“ Creating dataloaders...
2024-02-02 10:00:06 | âœ“ Creating model...
2024-02-02 10:00:06 |   Total parameters: 103,456,789
2024-02-02 10:00:07 | 
2024-02-02 10:00:07 | ======================================================================
2024-02-02 10:00:07 | TRAINING START
2024-02-02 10:00:07 | ======================================================================
2024-02-02 10:00:07 | 
2024-02-02 10:00:07 | Epoch 1/5
2024-02-02 10:05:30 | Train Loss: 0.2340
2024-02-02 10:06:15 | Val Loss: 0.1520
2024-02-02 10:06:15 | Val Precision: 0.9234
2024-02-02 10:06:15 | Val Recall: 0.9187
2024-02-02 10:06:15 | Val F1: 0.9210
2024-02-02 10:06:15 | âœ“ New best F1: 0.9210 - Model saved!
...
```

---

### 6. SLURM Log Structure

**File**: `logs/slurm_12345.out`

```
==========================================
Job ID: 12345
Job Name: sikubert_cnn
Node: gpu-node-01
Start Time: Fri Feb 02 10:00:00 2024
==========================================

==========================================
TRAINING CONFIGURATION
==========================================
Task: segmentation
Model: SIKU-BERT/sikubert
Batch Size: 64
Learning Rate: 2e-05
Epochs: 5
CNN Kernels: 3 5 7
CNN Filters: 256
==========================================

[Training output from train.py...]

==========================================
Job completed at: Fri Feb 02 12:30:00 2024
==========================================
```

**File**: `logs/slurm_12345.err`

```
[Error messages, warnings, or empty if no errors]
```

---

## ðŸ”„ Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Input Data                              â”‚
â”‚  (segmentation_train.json, val.json, test.json)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ClassicalChineseDataset                        â”‚
â”‚  â€¢ Tokenizes text character-by-character                    â”‚
â”‚  â€¢ Aligns labels with tokens                                â”‚
â”‚  â€¢ Handles padding and truncation                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   DataLoader                                â”‚
â”‚  â€¢ Batches data                                             â”‚
â”‚  â€¢ Shuffles training data                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         SikuBERTForTokenClassification                      â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚  SikuBERT    â”‚  Pretrained BERT for Classical Chinese   â”‚
â”‚  â”‚  Encoder     â”‚  Output: [batch, seq_len, 768]           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Dropout    â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚ Multi-Kernel â”‚  CNN with kernels [3, 5, 7]             â”‚
â”‚  â”‚     CNN      â”‚  Output: [batch, seq_len, 768]           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Dropout    â”‚                                           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â”‚         â”‚                                                   â”‚
â”‚         â–¼                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                           â”‚
â”‚  â”‚   Linear     â”‚  Classification head                     â”‚
â”‚  â”‚ Classifier   â”‚  Output: [batch, seq_len, num_labels]    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                Training Loop                                â”‚
â”‚  â€¢ Forward pass                                             â”‚
â”‚  â€¢ Compute CrossEntropyLoss                                 â”‚
â”‚  â€¢ Backward pass                                            â”‚
â”‚  â€¢ Optimizer step (AdamW)                                   â”‚
â”‚  â€¢ Learning rate scheduling                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Validation & Early Stopping                    â”‚
â”‚  â€¢ Evaluate on validation set                               â”‚
â”‚  â€¢ Calculate precision, recall, F1                          â”‚
â”‚  â€¢ Save best model                                          â”‚
â”‚  â€¢ Stop if no improvement for N epochs                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Test Evaluation                            â”‚
â”‚  â€¢ Load best model                                          â”‚
â”‚  â€¢ Evaluate on test set (never seen before)                 â”‚
â”‚  â€¢ Calculate final metrics                                  â”‚
â”‚  â€¢ Save results to JSON                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Outputs                                 â”‚
â”‚  â€¢ models/best_segmentation_model_cnn.pt                    â”‚
â”‚  â€¢ outputs/train_segmentation.log                           â”‚
â”‚  â€¢ outputs/segmentation_results.json                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸŽ¯ Task-Specific Label Structures

### Segmentation Task

**Labels**: `['B', 'M', 'E', 'S']`

```
Text:   å¤© åœ° çŽ„ é»ƒ å®‡ å®™ æ´ª è’
Labels: B  M  M  E  B  M  M  E

Meaning:
- B (Begin):  First character of a sentence
- M (Middle): Middle character of a sentence
- E (End):    Last character of a sentence
- S (Single): Single-character sentence
```

**Example Data**:
```json
{
  "text": "å¤©åœ°çŽ„é»ƒå®‡å®™æ´ªè’",
  "labels": ["B", "M", "M", "E", "B", "M", "M", "E"]
}
```

**Prediction Output**:
```
Segmented: å¤©åœ°çŽ„é»ƒ | å®‡å®™æ´ªè’
```

---

### Punctuation Task

**Labels**: `['O', 'ï¼Œ', 'ã€‚', 'ï¼š', 'ã€', 'ï¼›', 'ï¼Ÿ', 'ï¼']`

```
Text:   å¤© åœ° çŽ„ é»ƒ å®‡ å®™ æ´ª è’
Labels: O  O  O  ï¼Œ  O  O  O  ã€‚

Meaning:
- O:  No punctuation after this character
- ï¼Œ: Comma
- ã€‚: Period
- ï¼š: Colon
- ã€: Enumeration comma
- ï¼›: Semicolon
- ï¼Ÿ: Question mark
- ï¼: Exclamation mark
```

**Example Data**:
```json
{
  "text": "å¤©åœ°çŽ„é»ƒå®‡å®™æ´ªè’",
  "labels": ["O", "O", "O", "ï¼Œ", "O", "O", "O", "ã€‚"]
}
```

**Prediction Output**:
```
Punctuated: å¤©åœ°çŽ„é»ƒï¼Œå®‡å®™æ´ªè’ã€‚
```

---

## ðŸ“¦ Model Architecture Details

```
SikuBERTForTokenClassification(
  (bert): AutoModel(
    vocab_size: 21128
    hidden_size: 768
    num_hidden_layers: 12
    num_attention_heads: 12
    parameters: ~102M
  )
  
  (dropout): Dropout(p=0.1)
  
  (extra_layer): MultiKernelCNN(
    (convs): ModuleList(
      (0): Conv1d(768, 256, kernel_size=3, padding=1)
      (1): Conv1d(768, 256, kernel_size=5, padding=2)
      (2): Conv1d(768, 256, kernel_size=7, padding=3)
    )
    output_size: 768 (256 * 3 kernels)
    parameters: ~1.5M
  )
  
  (classifier): Linear(
    in_features: 768
    out_features: 4  (for segmentation) or 8 (for punctuation)
    parameters: ~3K
  )
)

Total Parameters: ~103.5M
Trainable Parameters: ~103.5M
```

---

## ðŸ”¢ Batch Processing Example

**Input Batch Shape**:
```python
{
  'input_ids': torch.Size([64, 256]),      # [batch_size, max_length]
  'attention_mask': torch.Size([64, 256]), # [batch_size, max_length]
  'labels': torch.Size([64, 256])          # [batch_size, max_length]
}
```

**Model Forward Pass**:
```python
Input: [64, 256] token IDs
  â†“
BERT Encoder: [64, 256, 768] hidden states
  â†“
Dropout: [64, 256, 768]
  â†“
Multi-Kernel CNN: [64, 256, 768]
  â†“
Dropout: [64, 256, 768]
  â†“
Linear Classifier: [64, 256, 4] logits
  â†“
Output: Predictions + Loss
```

---

## ðŸ’¾ Storage Requirements

| Component | Size | Notes |
|-----------|------|-------|
| Raw Data | ~10-100 MB | JSON files (depends on dataset size) |
| Model Checkpoint | ~400-500 MB | PyTorch state dict |
| Training Logs | ~1-10 MB | Text logs per experiment |
| SLURM Logs | ~1-5 MB | Per job |
| Cache (HuggingFace) | ~400 MB | Downloaded pretrained models |
| **Total** | **~1-2 GB** | Per experiment |

---

## ðŸš€ Execution Flow

```
1. User edits config.sh
   â””â”€> Sets data paths, hyperparameters
   
2. User submits: sbatch run_slurm_with_config.sh
   â””â”€> SLURM schedules job on GPU node
   
3. Job starts on compute node
   â”œâ”€> Loads environment (modules, virtualenv)
   â”œâ”€> Sources config.sh
   â”œâ”€> Creates directories (logs/, outputs/, models/)
   â””â”€> Executes: python train.py [args...]
   
4. train.py execution
   â”œâ”€> Setup logging
   â”œâ”€> Load tokenizer
   â”œâ”€> Load and preprocess data
   â”œâ”€> Create dataloaders
   â”œâ”€> Initialize model
   â”œâ”€> Training loop
   â”‚   â”œâ”€> Train epoch
   â”‚   â”œâ”€> Validate
   â”‚   â”œâ”€> Save best model
   â”‚   â””â”€> Early stopping check
   â”œâ”€> Load best model
   â”œâ”€> Test evaluation
   â””â”€> Save results
   
5. Job completes
   â”œâ”€> Outputs saved to outputs/
   â”œâ”€> Model saved to models/
   â””â”€> Logs saved to logs/
   
6. User checks results
   â””â”€> ./slurm_helper.sh logs <job_id>
```

---

## ðŸ“ File Naming Conventions

```
Training Logs:    train_{task}.log
Model Files:      best_{task}_model_cnn.pt
Result Files:     {task}_results.json
SLURM Logs:       slurm_{job_id}.out/err
Prediction Files: test_pred.json
```

---

## âœ… Data Validation Checklist

Before training, ensure:

- [ ] Data files exist at specified paths
- [ ] JSON format is valid
- [ ] `len(text) == len(labels)` for all samples
- [ ] Labels match task configuration
- [ ] Files are UTF-8 encoded
- [ ] Train/val/test splits are separate
- [ ] No data leakage between splits
- [ ] Reasonable dataset size (>1000 samples recommended)

---

This structure provides a complete overview of how data flows through the system and where everything is stored.
